# DataFlowEngineering360 - Pipeline Big Data Maritime

![Architecture](architecture_dataflow360_final_paper_1764534128793.png)

## Pr√©sentation

**DataFlowEngineering360** est une plateforme compl√®te de traitement de donn√©es con√ßue pour g√©rer √† la fois des flux **batch** et **temps r√©el** dans le contexte maritime portuaire. Le projet simule un environnement professionnel de bout en bout, int√©grant la g√©n√©ration, l‚Äôingestion, le stockage, le traitement, le monitoring et l‚Äôexploitation des donn√©es du Port Autonome de Dakar.

Ce projet a √©t√© r√©alis√© par **Alla NIANG**, apprenant en D√©veloppement DATA √† **ODC (Orange Digital Center), Promo 7**.

## √âtat d'avancement

üöß **Projet en cours de d√©veloppement : 85%**

Le pipeline end-to-end est fonctionnel. Les travaux restants concernent principalement l'optimisation des transformations, l'enrichissement des dashboards et la mise en place compl√®te du pipeline CI/CD.

## Objectifs du projet

- **Pipeline Hybride** : Cr√©er un pipeline de donn√©es capable de traiter des flux batch (historiques) et streaming (temps r√©el).
- **Architecture Moderne** : Mettre en ≈ìuvre les bonnes pratiques du Data Engineering (Data Lake, Data Warehouse, ELT/ETL).
- **Infrastructure** : Conteneuriser l‚Äôarchitecture compl√®te via Docker pour une portabilit√© maximale.
- **Analytique** : Fournir des outils de BI et de Data Science pour l'aide √† la d√©cision.

## Fonctionnalit√©s cl√©s

- **G√©n√©ration de donn√©es** : Simulation de trafic maritime, donn√©es m√©t√©orologiques et logistiques (Python, Faker).
- **Ingestion multiformat** : Support de fichiers CSV, JSON, Excel, XML, YAML et flux API.
- **Stockage h√©t√©rog√®ne** :
    - **Data Lake** : HDFS (via Hadoop) pour le stockage brut.
    - **NoSQL** : MongoDB (documents), Cassandra (s√©ries temporelles), Neo4j (graphes).
    - **Data Warehouse** : PostgreSQL pour les donn√©es structur√©es et mod√©lis√©es (sch√©ma en √©toile).
- **Orchestration** : Apache Airflow pour la gestion des workflows batch.
- **Streaming** : Apache Kafka pour le traitement des √©v√©nements en temps r√©el.
- **Monitoring** : Stack ELK (Elasticsearch, Logstash, Kibana) et Grafana pour la supervision de l'infrastructure et des flux.
- **Valorisation** : Dashboards interactifs pour le suivi des KPIs portuaires.

## Contr√¥le Qualit√© des Donn√©es

üöß **En cours d'impl√©mentation**

Le projet int√®gre **Great Expectations** pour garantir la fiabilit√© et la qualit√© des donn√©es √† chaque √©tape du pipeline. Des validations sont effectu√©es pour v√©rifier :
- La conformit√© des sch√©mas.
- La validit√© des valeurs (nulls, types, plages).
- La coh√©rence temporelle des donn√©es.

## Architecture

![Architecture DataFlow360](architecture_dataflow360_final_paper_1764534128793.png)

L'architecture est con√ßue pour √™tre **modulaire et √©volutive**. Elle permet d'ajouter facilement de nouvelles sources de donn√©es ou de nouveaux modules de traitement sans impacter l'existant.

Le pipeline est structur√© autour de plusieurs composants :

```
DataFlow_Engineering360/
‚îú‚îÄ‚îÄ src/                  # Code source du projet
‚îÇ   ‚îú‚îÄ‚îÄ collection/       # Scripts de collecte
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/        # Pipelines Spark et Kafka
‚îÇ   ‚îú‚îÄ‚îÄ transformation/   # Transformations de donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ orchestration/    # DAGs Airflow
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/       # Configuration ELK
‚îÇ   ‚îî‚îÄ‚îÄ utils/            # Fonctions utilitaires
‚îú‚îÄ‚îÄ docker-compose*.yml   # Fichiers d'orchestration Docker
‚îú‚îÄ‚îÄ scripts/              # Scripts de d√©marrage/arr√™t
‚îî‚îÄ‚îÄ ...
```

### Technologies utilis√©es

| Domaine             | Outils                         |
|---------------------|-------------------------------|
| **Langages**        | Python, SQL, Shell            |
| **G√©n√©ration**      | Faker, Pandas, Requests       |
| **Ingestion**       | Apache Kafka, Spark Streaming |
| **Traitement**      | Apache Spark (PySpark)        |
| **Orchestration**   | Apache Airflow                |
| **Stockage**        | PostgreSQL, MongoDB, HDFS     |
| **Monitoring**      | Elasticsearch, Logstash, Kibana, Grafana |
| **Infrastructure**  | Docker, Docker Compose        |

## Installation et D√©marrage

### Pr√©requis

- Docker et Docker Compose install√©s.
- Une machine avec suffisamment de RAM (recommand√© : 16GB+) car la stack compl√®te est cons√©quente.

### Installation

1.  Cloner le d√©p√¥t :
    ```bash
    git clone https://github.com/niangalla/DataFlowEngineering360_BigData_Maritime_Pipeline.git
    cd DataFlowEngineering360_BigData_Maritime_Pipeline
    ```

2.  Configurer l'environnement :
    - Copier le fichier `.env.example` (si pr√©sent) vers `.env` et ajuster les variables si n√©cessaire.

### Lancement

Le projet utilise des scripts pour faciliter le d√©marrage des nombreux services :

```bash
# D√©marrer tous les services
./start-all.sh

# Arr√™ter tous les services
./stop-all.sh
```

Vous pouvez √©galement lancer des modules sp√©cifiques via Docker Compose :

```bash
docker-compose -f docker-compose.core.yml up -d
docker-compose -f docker-compose.airflow.yml up -d
# ... autres fichiers compose
```

## Auteur

**Alla NIANG**
- **Email** : niangalla98@gmail.com
- **Formation** : D√©veloppement Data, Orange Digital Center (Promo 7)

---
*Ce projet est r√©alis√© dans un but p√©dagogique et de d√©monstration de comp√©tences en Data Engineering.*
